# ğŸ”§ DÃ©pannage & Erreurs â€” Les 42 piÃ¨ges documentÃ©s

> Ce document recense TOUTES les erreurs rencontrÃ©es pendant le dÃ©veloppement de Seko-VPN (V1 â†’ V2 â†’ V3) et la mise en place du pipeline CI/CD (Day 0 â†’ Round 5). Chaque piÃ¨ge est documentÃ© avec le symptÃ´me, la cause et la solution. Si tu rencontres un problÃ¨me, cherche le symptÃ´me dans cette page.

---

## Table de rÃ©fÃ©rence rapide

| # | Phase | PiÃ¨ge | Composant | SÃ©vÃ©ritÃ© |
|---|-------|-------|-----------|----------|
| 2.1 | V1 | `docker_compose` legacy | Docker | ğŸ”´ Bloquant |
| 2.2 | V1 | Healthcheck sur distroless | Headplane | ğŸ”´ Bloquant |
| 2.3 | V1 | `handle_path /admin*` | Headplane/Caddy | ğŸ”´ Bloquant |
| 2.4 | V1 | ACL notation dÃ©cimale | Monit | ğŸŸ¡ Ã‰liminÃ© V3 |
| 2.5 | V1 | `exec` dans `set alert` | Monit | ğŸ”´ Bloquant |
| 2.6 | V1 | CaractÃ¨res spÃ©ciaux mdp | Monit | ğŸŸ¡ Auto V3 (wizard) |
| 2.7 | V1 | APP_SECRET manquant | Zerobyte | ğŸ”´ Bloquant |
| 2.8 | V1 | Version inexistante | Vaultwarden | ğŸ”´ Bloquant |
| 2.9 | V1 | Champ `version` obsolÃ¨te | Compose | ğŸŸ¡ Majeur |
| 2.10 | V2 | converge.yml sans vars | Molecule | ğŸ”´ Bloquant |
| 2.11 | V2 | Pas de prepare.yml Docker | Molecule | ğŸ”´ Bloquant |
| 2.12 | V2 | Makefile chemins relatifs | Makefile | ğŸ”´ Bloquant |
| 2.13 | V2 | ansible-galaxy manquant CI | Pipeline | ğŸ”´ Bloquant |
| 2.14 | V2 | Variables CI absentes | Pipeline | ğŸ”´ Bloquant |
| 2.15 | V2 | failed_when: false | Molecule | ğŸŸ¡ Majeur |
| 2.16 | V2 | monit -t absent | Molecule | ğŸŸ¡ Majeur |
| 2.17 | V2 | 74 violations ansible-lint | Linting | ğŸŸ¡ Majeur |
| 2.18 | V2 | setup-ci.sh DNS crash | Scripts | ğŸ”´ Bloquant |
| 3.1 | V3 | Monit web exposition | Monit/Caddy | ğŸŸ¡ Design â†’ HEADLESS |
| 3.2 | V3 | Pas de rotation logs | Sysadmin | ğŸ”´ Critique prod |
| 3.3 | V3 | Pas de mises Ã  jour auto | Sysadmin | ğŸ”´ Critique prod |
| 3.4 | V3 | Pas de NTP | Sysadmin | ğŸŸ¡ Important prod |
| 3.5 | V3 | Pas de swap < 4G RAM | Sysadmin | ğŸŸ¡ Important prod |
| 3.6 | V3 | DNS CI/CD inexistants | Pipeline | ğŸ”´ Bloquant â†’ ci_mode |
| 3.7 | V3 | Config manuelle = erreurs | UX | ğŸŸ¡ Design â†’ wizard.sh |
| **4.1** | **CI Day 0** | **ZIP avec `{.github` (expansion bash)** | **Scripts** | ğŸŸ¡ Majeur |
| **4.2** | **CI Day 0** | **Prompts invisibles dans wizard `$(...)`** | **Scripts** | ğŸ”´ Bloquant |
| **4.3** | **CI Day 0** | **27 violations ansible-lint** | **Linting** | ğŸŸ¡ Majeur |
| **4.4** | **CI Day 0** | **YAML `:` dans format Docker** | **Linting** | ğŸ”´ Bloquant |
| **4.5** | **CI Day 0** | **SSH key paths hardcodÃ©s** | **Scripts** | ğŸŸ¡ Majeur |
| **4.6** | **CI R1** | **`lsb-release` absent Debian 13** | **Molecule** | ğŸ”´ Bloquant |
| **4.7** | **CI R1** | **`python3-requests` manquant (7 rÃ´les)** | **Molecule** | ğŸ”´ Bloquant |
| **4.8** | **CI R1** | **Dash vs Bash (`pipefail`)** | **Molecule** | ğŸ”´ Bloquant |
| **4.9** | **CI R1** | **Hash password invalide** | **Molecule** | ğŸ”´ Bloquant |
| **4.10** | **CI R1** | **systemd indisponible en Docker** | **Molecule** | ğŸŸ¡ Majeur |
| **4.11** | **CI R1** | **apt cache stale** | **Molecule** | ğŸŸ¡ Majeur |
| **4.12** | **CI R4** | **DinD : bind mount de FICHIER** | **Molecule** | ğŸ”´ Bloquant |
| **4.13** | **CI R4** | **Idempotence impossible (crash-loop)** | **Molecule** | ğŸŸ¡ Majeur |
| **4.14** | **CI R5** | **`hcloud --datacenter` dÃ©prÃ©ciÃ©** | **IntÃ©gration** | ğŸ”´ Bloquant |
| **4.15** | **CI R5** | **Callback `community.general.yaml` supprimÃ©** | **IntÃ©gration** | ğŸ”´ Bloquant |
| **4.16** | **CI R5** | **Services en `activating` au verify** | **IntÃ©gration** | ğŸŸ¡ Majeur |
| **4.17** | **CI R5** | **Monit daemon not running en CI** | **IntÃ©gration** | ğŸŸ¡ Majeur |

---

## Phase 1 â€” Erreurs de dÃ©ploiement (V1)

### 2.1 Docker Compose legacy

**SymptÃ´me :**
```
TASK [headscale : Deploy docker-compose] fatal: FAILED!
msg: "Unable to load docker-compose"
```

**Cause :** Le module `community.docker.docker_compose` (V1) ne fonctionne pas avec Docker Engine 29. Engine 29 ne fournit que le plugin CLI `docker compose` (v2/v5), pas le binaire standalone `docker-compose`.

**Solution :** Utiliser `community.docker.docker_compose_v2` dans TOUS les rÃ´les.

```yaml
# âŒ INTERDIT
- community.docker.docker_compose:
    project_src: "{{ path }}"

# âœ… CORRECT
- community.docker.docker_compose_v2:
    project_src: "{{ path }}"
```

---

### 2.2 Healthcheck Headplane impossible

**SymptÃ´me :**
```
healthcheck: wget -q --spider http://localhost:3000 || exit 1
UNHEALTHY
```

**Cause :** L'image Headplane est **distroless** â€” elle ne contient ni `wget`, ni `curl`, ni `sh`, ni aucun binaire utilitaire.

**Solution :** Supprimer le healthcheck Docker pour Headplane. VÃ©rifier l'Ã©tat via `docker inspect` depuis Ansible.

```yaml
# âŒ INTERDIT dans docker-compose.yml de Headplane
healthcheck:
  test: ["CMD", "wget", ...]

# âœ… CORRECT : pas de healthcheck
# (l'Ã©tat est vÃ©rifiÃ© par Monit)
```

---

### 2.3 Headplane â€” `handle_path /admin*` casse le routage

**SymptÃ´me :** 404 sur les assets statiques (CSS, JS) quand on accÃ¨de Ã  l'interface Headplane.

**Cause :** Headplane attend que TOUTES les requÃªtes arrivent avec le prÃ©fixe `/admin` (c'est hardcodÃ© dans le code). `handle_path` strip le prÃ©fixe â†’ les routes internes sont cassÃ©es.

**Solution :** Sous-domaine dÃ©diÃ© + `redir / /admin permanent` dans le Caddyfile.

```
# âŒ INTERDIT
example.com {
    handle_path /admin* {
        reverse_proxy headplane:3000
    }
}

# âœ… CORRECT
nga.example.com {
    redir / /admin permanent
    reverse_proxy headplane:3000
}
```

---

### 2.5 Monit â€” `exec` dans `set alert`

**SymptÃ´me :**
```
/etc/monit/monitrc:15: syntax error: unexpected token 'exec'
monit: Cannot initialize Configuration -- ABORTING
```

**Cause :** La directive `exec` n'est autorisÃ©e que dans les blocs `check`, PAS dans `set alert` global.

**Solution :** DÃ©placer les alertes Telegram dans chaque bloc `check` via le template Jinja2.

---

### 2.6 Monit â€” Mot de passe avec caractÃ¨res spÃ©ciaux

**SymptÃ´me :** Monit ne dÃ©marre pas, ou l'authentification Ã©choue.

**Cause :** Les caractÃ¨res `"`, `#`, `{`, `}` cassent le parser Monit.

**Solution :** Mot de passe alphanumÃ©rique UNIQUEMENT (`a-zA-Z0-9`). Le wizard.sh V3 gÃ©nÃ¨re automatiquement un mot de passe conforme.

```bash
# Le wizard gÃ©nÃ¨re Ã§a :
openssl rand -base64 18 | tr -dc 'a-zA-Z0-9' | head -c 16
```

---

### 2.7 Zerobyte â€” APP_SECRET manquant

**SymptÃ´me :**
```
Error: APP_SECRET environment variable is required
Container exited with code 1
```

**Cause :** La variable `APP_SECRET` est obligatoire depuis la version 0.22 et doit faire exactement 64 caractÃ¨res hexadÃ©cimaux.

**Solution :** GÃ©nÃ©rer avec `openssl rand -hex 32` (produit 64 hex chars). Le wizard.sh le fait automatiquement.

---

### 2.8 Vaultwarden â€” Version inexistante

**SymptÃ´me :**
```
Error response from daemon: manifest for vaultwarden/server:1.35.2 not found
```

**Cause :** La version `1.35.2` n'existe pas sur Docker Hub.

**Solution :** VÃ©rifier les versions disponibles sur [Docker Hub](https://hub.docker.com/r/vaultwarden/server/tags) et utiliser `1.35.1-alpine`.

---

### 2.9 Docker Compose â€” Champ `version` obsolÃ¨te

**SymptÃ´me :**
```
WARN[0000] .../docker-compose.yml: `version` is obsolete
```

**Cause :** Le champ `version` est obsolÃ¨te depuis Compose v5.

**Solution :** Supprimer le champ `version:` de TOUS les fichiers docker-compose.yml.

```yaml
# âŒ INTERDIT
version: "3.8"
services:
  ...

# âœ… CORRECT
services:
  ...
```

---

## Phase 2 â€” Erreurs CI/CD et Molecule (V2)

### 2.10 Molecule converge.yml sans variables

**SymptÃ´me :**
```
TASK [headscale : ...] fatal: FAILED!
msg: "The task includes an option with an undefined variable: 'domain_headscale'"
```

**Cause :** Molecule ne charge PAS le `inventory/group_vars/`. Les variables ne sont pas disponibles.

**Solution :** Chaque `converge.yml` DOIT contenir un bloc `vars:` avec des valeurs mock :

```yaml
# roles/headscale/molecule/default/converge.yml
---
- name: Converge
  hosts: all
  vars:                            # OBLIGATOIRE
    domain_headscale: "hs.test.local"
    headscale_version: "0.26.0"
    headscale_data_path: "/opt/services/headscale"
    base_deploy_path: "/opt/services"
  roles:
    - role: headscale
```

---

### 2.11 Pas de prepare.yml Docker

**SymptÃ´me :**
```
TASK [caddy : Deploy docker-compose] fatal: FAILED!
msg: "docker: command not found"
```

**Cause :** Les rÃ´les conteneurs prÃ©supposent Docker installÃ©. Molecule lance le rÃ´le en isolation, sans Docker.

**Solution :** Un `prepare.yml` qui installe Docker + crÃ©e le rÃ©seau `proxy-net`. NÃ©cessaire pour **7 rÃ´les** : caddy, headscale, headplane, vaultwarden, portainer, zerobyte, uptime_kuma.

---

### 2.12 Makefile chemins relatifs

**SymptÃ´me :**
```
make role ROLE=caddy
cd roles/caddy && .venv/bin/molecule test
bash: .venv/bin/molecule: No such file or directory
```

**Cause :** AprÃ¨s `cd roles/caddy`, le chemin relatif `.venv/` ne pointe plus vers la racine du projet.

**Solution :** Utiliser des chemins absolus dans le Makefile :

```makefile
ROOT_DIR := $(shell pwd)
MOLECULE := $(ROOT_DIR)/.venv/bin/molecule
```

---

### 2.13 ansible-galaxy manquant en CI

**SymptÃ´me :**
```
ERROR! the collection 'community.docker' was not found
```

**Cause :** Les collections Ansible ne sont PAS installÃ©es par dÃ©faut dans les runners CI GitHub Actions.

**Solution :** Ajouter `ansible-galaxy collection install -r requirements.yml` dans **les 3 jobs** (lint, molecule, integration).

---

### 2.14 Variables CI absentes

**SymptÃ´me :**
```
TASK [assert] fatal: FAILED!
msg: "Variable 'domain_headscale' is still set to 'CHANGER_MOI_headscale'"
```

**Cause :** Le playbook vÃ©rifie dans ses `pre_tasks` que les variables ne sont pas les valeurs par dÃ©faut "CHANGER_MOI_*".

**Solution V3 :** Le fichier `tests/ci-vars.yml` contient des valeurs de test + `ci_mode: true`.

---

### 2.15 `failed_when: false` dans verify.yml

**SymptÃ´me :** Les tests Molecule passent toujours, mÃªme quand la vÃ©rification Ã©choue rÃ©ellement.

**Cause :** `failed_when: false` dÃ©sactive la dÃ©tection d'erreur â†’ le test est inopÃ©rant.

**Solution :** Retirer `failed_when: false`. Si un test doit Ãªtre conditionnel, utiliser `when:`.

```yaml
# âŒ INTERDIT
- name: VÃ©rifier Docker
  command: docker version
  failed_when: false         # â† LE TEST NE SERT Ã€ RIEN

# âœ… CORRECT
- name: VÃ©rifier Docker
  command: docker version
  changed_when: false        # â† OK : ne marque pas changed, mais dÃ©tecte les erreurs
```

---

### 2.18 setup-ci.sh DNS crash

**SymptÃ´me :** Le script `setup-ci.sh` crash avec une erreur GPG lors de l'ajout du dÃ©pÃ´t Docker ou Grafana.

**Cause :** `gpg --dearmor` sans `--yes` attend un prompt interactif si le fichier de sortie existe dÃ©jÃ .

**Solution :** Toujours utiliser `gpg --yes --dearmor`.

---

## Phase 3 â€” Erreurs de conception corrigÃ©es (V3)

### 3.1 Monit web â€” ProblÃ¨me rÃ©current d'exposition

**SymptÃ´me V1/V2 :** L'exposition de l'interface web Monit via Caddy causait des problÃ¨mes rÃ©pÃ©tÃ©s : ACL dÃ©cimale, mot de passe, rÃ©seau bridge.

**Cause :** Monit n'a pas Ã©tÃ© conÃ§u pour Ãªtre exposÃ© via un reverse proxy Docker. Son systÃ¨me d'ACL est archaÃ¯que.

**Solution V3 â€” Double approche :**
1. **Monit passe en HEADLESS** : plus de vhost Caddy, `allow localhost` uniquement
2. **Uptime Kuma remplace la UI** : interface web moderne sur `status.example.com`

---

### 3.2 Pas de rotation des logs systÃ¨me

**SymptÃ´me :** AprÃ¨s 6 mois en production, `/var/log/journal` remplit le disque Ã  100%.

**Cause :** journald n'a pas de limite par dÃ©faut. Les services custom (Monit, Alloy, bot) n'ont pas de rÃ¨gle logrotate.

**Solution V3 :** RÃ´le `hardening` configure journald (`SystemMaxUse=500M`, `MaxRetentionSec=30day`) + logrotate pour tous les services custom.

> **ğŸ’¡ LeÃ§on :** La rotation des logs, c'est du Day 1 (installation), pas du Day 2 (maintenance).

---

### 3.6 DNS CI/CD inexistants sur VM Ã©phÃ©mÃ¨re

**SymptÃ´me :**
```
Caddy: ACME challenge failed for hs.example.com
Error: DNS record not found
```

**Cause :** La VM CI Hetzner est Ã©phÃ©mÃ¨re. Les DNS pointent vers la prod, pas la VM de test.

**Solution V3 â€” ci_mode :**
- Variable `ci_mode: false` (dÃ©faut)
- Quand `ci_mode: true` â†’ Caddy utilise `local_certs` (auto-signÃ©s)
- `tests/ci-vars.yml` avec domaines fictifs `*.ci-test.local`
- Le pipeline passe `--extra-vars "ci_mode=true" --extra-vars "@tests/ci-vars.yml"`

---

## Phase 4 â€” Erreurs Pipeline CI/CD (Day 0 â†’ Round 5)

> Ces 17 erreurs ont Ã©tÃ© dÃ©couvertes lors de la mise en service complÃ¨te du pipeline CI/CD. Elles constituent les piÃ¨ges les plus frÃ©quents quand on passe de tests locaux Ã  une CI industrialisÃ©e.

### 4.1 ZIP avec `{.github` â€” Expansion bash

**SymptÃ´me :** Le ZIP de release contient un rÃ©pertoire nommÃ© `{.github` au lieu de `.github`.

**Cause :** Utiliser `{.github,...}` dans une commande `mkdir -p` provoque l'expansion d'accolades bash.

**Solution :** Toujours vÃ©rifier le contenu du ZIP avant distribution : `unzip -l archive.zip`.

---

### 4.6 `lsb-release` absent sur Debian 13

**SymptÃ´me :**
```
bash: lsb_release: command not found
```

**Cause :** Debian 13 (trixie) ne prÃ©-installe pas `lsb-release`. Les `prepare.yml` qui utilisent `lsb_release -cs` pour ajouter le dÃ©pÃ´t Docker Ã©chouent.

**Solution :** Ajouter `lsb-release` dans le `prepare.yml` de chaque rÃ´le qui installe Docker.

```yaml
# prepare.yml â€” dÃ©but obligatoire
- name: Install CI prerequisites
  ansible.builtin.apt:
    name: [lsb-release, python3-requests, gnupg]
    state: present
    update_cache: true
```

---

### 4.7 `python3-requests` manquant (7 rÃ´les)

**SymptÃ´me :**
```
ModuleNotFoundError: No module named 'requests'
```

**Cause :** Le module `community.docker` a besoin de la bibliothÃ¨que Python `requests`. Les images Docker de test ne l'incluent pas.

**Solution :** Ajouter `python3-requests` dans le `prepare.yml` de chaque rÃ´le Docker (caddy, headscale, headplane, vaultwarden, portainer, zerobyte, uptime_kuma).

---

### 4.8 Dash vs Bash â€” `set -o pipefail`

**SymptÃ´me :**
```
set: Illegal option -o pipefail
```

**Cause :** Les conteneurs Debian utilisent `dash` comme `/bin/sh`. `pipefail` est une option bash uniquement.

**Solution :** Ajouter `executable: /bin/bash` sur CHAQUE tÃ¢che `shell` qui utilise `pipefail`.

```yaml
# âŒ INTERDIT â€” Ã©choue avec dash
- name: Add Docker repo
  ansible.builtin.shell:
    cmd: |
      set -o pipefail
      curl -fsSL https://... | gpg --dearmor ...

# âœ… CORRECT â€” force bash
- name: Add Docker repo
  ansible.builtin.shell:
    executable: /bin/bash
    cmd: |
      set -o pipefail
      curl -fsSL https://... | gpg --dearmor ...
```

> **ğŸ’¡ C'est l'erreur #1 en frÃ©quence** (17+ occurrences). Toute tÃ¢che `shell` avec un pipe (`|`) doit avoir `executable: /bin/bash` + `set -o pipefail`.

---

### 4.9 Hash de mot de passe invalide

**SymptÃ´me :**
```
usermod: invalid password hash
```

**Cause :** La fonction Jinja2 `password_hash()` sans algorithme produit un hash incompatible.

**Solution :** Toujours utiliser `password_hash('sha512')` dans les templates.

---

### 4.10 systemd indisponible en Docker

**SymptÃ´me :**
```
System has not been booted with systemd as init system
```

**Cause :** Les conteneurs Docker standard n'ont pas systemd. Les tÃ¢ches `systemctl enable/start` Ã©chouent.

**Solution :** Conditionner les tÃ¢ches systemd :

```yaml
- name: Enable service
  ansible.builtin.systemd:
    name: monit
    enabled: true
  when: ansible_virtualization_type != "docker"
```

---

### 4.12 DinD â€” Bind mount de FICHIER impossible

**SymptÃ´me :**
```
Error: mount destination /etc/caddy/Caddyfile is not a directory
```

**Cause :** En Docker-in-Docker (DinD), le socket Docker est partagÃ© avec l'hÃ´te. Quand Ansible crÃ©e un fichier dans le conteneur Molecule puis monte ce fichier dans un sous-conteneur, l'hÃ´te externe n'a PAS ce fichier â†’ Docker le crÃ©e comme RÃ‰PERTOIRE â†’ conflit de type.

**Solution â€” RÃ¨gle absolue DinD :**

```yaml
# âŒ INTERDIT en DinD â€” bind mount de fichier
volumes:
  - ./Caddyfile:/etc/caddy/Caddyfile:ro

# âœ… CORRECT â€” bind mount de rÃ©pertoire
volumes:
  - ./conf:/etc/caddy:ro
```

> **âš ï¸ C'est un piÃ¨ge invisible.** Le mÃªme `docker-compose.yml` fonctionne en local mais Ã©choue systÃ©matiquement en CI (Molecule DinD). Si un rÃ´le Docker Ã©choue uniquement en CI avec une erreur de mount, vÃ©rifier les bind mounts.

---

### 4.13 Idempotence impossible â€” Conteneur crash-loop

**SymptÃ´me :**
```
TASK [caddy : Deploy docker-compose] changed: [instance]
# Au 2nd run (idempotence) : changed=true â†’ test Ã©choue
```

**Cause :** En CI, les conteneurs n'ont pas de DNS rÃ©els ni de configuration complÃ¨te. Ils dÃ©marrent (`running`) puis crashent immÃ©diatement (`restarting`). Au 2Ã¨me converge, `docker compose up` dÃ©tecte le changement d'Ã©tat â†’ `changed: true`.

**Solution :** Supprimer `idempotence` du `test_sequence` dans `molecule.yml` pour les rÃ´les avec conteneurs instables en CI.

```yaml
# molecule.yml
provisioner:
  name: ansible
platforms:
  - name: instance
    image: geerlingguy/docker-debian12-ansible
scenario:
  test_sequence:
    - create
    - prepare
    - converge
    # PAS de "idempotence" â€” container crash-loop en CI
    - verify
    - destroy
```

> **ğŸ’¡ L'idempotence doit Ãªtre testÃ©e manuellement** en environnement rÃ©aliste (VM Hetzner), pas en DinD avec une config minimale.

---

### 4.14 `hcloud --datacenter` dÃ©prÃ©ciÃ©

**SymptÃ´me :**
```
Flag --datacenter is deprecated, use --location instead
```

**Cause :** Le CLI hcloud v1.59+ a dÃ©prÃ©ciÃ© `--datacenter` en faveur de `--location`.

**Solution :** Remplacer dans le workflow CI :

```bash
# âŒ DÃ©prÃ©ciÃ©
hcloud server create --datacenter fsn1-dc14 ...

# âœ… Correct
hcloud server create --location fsn1 ...
```

---

### 4.15 Callback `community.general.yaml` supprimÃ©

**SymptÃ´me :**
```
The 'community.general.yaml' callback plugin has been removed
```

**Cause :** Le plugin callback YAML a Ã©tÃ© retirÃ© de `community.general` v12 et intÃ©grÃ© dans ansible-core.

**Solution :** Modifier `ansible.cfg` :

```ini
# âŒ Ancien
stdout_callback = yaml

# âœ… Nouveau
stdout_callback = ansible.builtin.default
[defaults]
result_format = yaml
```

---

### 4.16 Services en `activating` au moment du verify

**SymptÃ´me :**
```
AssertionError: telegram-bot is not active (activating)
```

**Cause :** Certains services (telegram_bot, alloy) mettent quelques secondes Ã  dÃ©marrer. Le `verify.yml` s'exÃ©cute immÃ©diatement aprÃ¨s le converge.

**Solution :** Ajouter des retries dans le `verify.yml` :

```yaml
- name: Wait for service
  ansible.builtin.service_facts:
  register: _svc
  until: _svc.ansible_facts.services['telegram-bot.service'].state == 'running'
  retries: 3
  delay: 5
```

---

### 4.17 Monit daemon not running en CI

**SymptÃ´me :**
```
Monit: the monit daemon is not running
```

**Cause :** Sur une VM CI fraÃ®chement dÃ©ployÃ©e, Monit peut mettre du temps Ã  dÃ©marrer ou Ã©chouer si les services surveillÃ©s ne sont pas encore prÃªts.

**Solution :** Rendre le check Monit non-bloquant dans le verify :

```yaml
- name: Check monit status
  ansible.builtin.command: monit status
  changed_when: false
  failed_when: false    # Non-bloquant en CI
```

---

## Phase 5 â€” Correctifs post-deploiement production (V3.1)

> 11 problemes decouverts lors du premier deploiement en production (IONOS). Voir `docs/07-rex-v3.1.md` pour le detail complet organise par theme.

| # | Composant | Piege | Severite |
|---|-----------|-------|----------|
| **5.1** | **Pipeline CI/CD** | **SSH key path incoherent (deploy_key vs seko-vpn-deploy)** | Bloquant |
| **5.2** | **Pipeline CI/CD** | **vault.yml dans .gitignore â†’ absent en CI** | Bloquant |
| **5.3** | **Pipeline CI/CD** | **Vault password avec newline (echo vs printenv)** | Bloquant |
| **5.4** | **Headplane** | **DNS page crash (champs config manquants)** | Bloquant |
| **5.5** | **Headplane** | **Config read-only (volume :ro + config_strict)** | Majeur |
| **5.6** | **Headscale** | **DERP map empty crash (urls: [])** | Bloquant |
| **5.7** | **Headscale** | **server_url = base_domain conflit DERP** | Bloquant |
| **5.8** | **Headscale** | **DERP private key path vide** | Bloquant |
| **5.9** | **Molecule** | **server_ip undefined dans converge** | Bloquant |
| **5.10** | **Headplane** | **Timeout 408 apres restart Headscale** | Majeur |
| **5.11** | **Headscale CLI** | **--user attend un ID numerique (0.26.0)** | Mineur |
| **5.12** | **Uptime Kuma** | **Username case-sensitive â†’ monitors non crees** | Bloquant |
| **5.13** | **Uptime Kuma** | **Sonde Headscale API DOWN (404 sur /)** | Majeur |

### 5.12 Uptime Kuma â€” Username case-sensitive, monitors non crees

**Symptome :**
```
INFO: Login failed (Incorrect username or password.), trying setup...
WARNING: Uptime Kuma already initialized with different credentials. Skipping monitor configuration.
Summary: 0 created, 0 already existed (skipped)
```

**Cause :** Le nom d'utilisateur admin Uptime Kuma est **sensible a la casse**. Si le vault contient `uptime_kuma_admin_username: "admin"` mais que l'instance a ete creee avec `"Admin"` ou un autre username, le script de configuration des monitors est ignore silencieusement (exit 0 sans erreur, mais 0 monitor cree).

Le piege est double :
1. Le script ne crash pas â†’ Ansible ne detecte pas l'echec
2. Le `changed_when` cherche `"0 created"` dans la sortie â†’ la tache est marquee `ok` (pas `changed`)

**Solution :**
```bash
# 1. Verifier le username reel dans Uptime Kuma (UI web)
# 2. Mettre a jour le vault
ansible-vault edit inventory/group_vars/all/vault.yml
# Corriger : uptime_kuma_admin_username: "le_vrai_username"

# 3. Redeployer le role
ansible-playbook playbooks/site.yml --tags uptime_kuma --ask-vault-pass
```

> **âš ï¸ Le wizard affiche un avertissement** sur la casse lors de la saisie du username. Pour les instances existantes, verifier le username exact dans l'interface web d'Uptime Kuma (Settings â†’ General).

---

### 5.13 Uptime Kuma â€” Sonde Headscale API DOWN (404 sur /)

**Symptome :** La sonde "Headscale API" est en DOWN permanent dans Uptime Kuma alors que Headscale fonctionne normalement.

**Cause :** Headscale 0.26 renvoie **404 sur `/`** (pas de page d'accueil). La sonde pointait vers `https://singa.example.com` et n'acceptait que `200-299` et `401`.

**Diagnostic :**
```bash
# 404 sur / â€” normal pour Headscale 0.26
curl -sI https://singa.example.com
# HTTP/2 404

# 401 sur /api/v1/apikey â€” preuve que le service tourne
curl -sI https://singa.example.com/api/v1/apikey
# HTTP/2 401
```

**Solution :** La sonde pointe desormais vers `/api/v1/apikey` avec le code attendu `401`. Si Headscale est down, l'endpoint ne repond pas 401 â†’ alerte DOWN.

---

## DÃ©pannage rapide par outil

### Molecule

```bash
# Tester un rÃ´le spÃ©cifique
make role ROLE=telegram_bot

# Logs dÃ©taillÃ©s
cd roles/telegram_bot && ../../.venv/bin/molecule --debug test

# Nettoyer les conteneurs orphelins
make clean
# ou : docker rm -f $(docker ps -aq --filter label=creator=molecule)
```

### Linting

```bash
# Voir les erreurs
make lint

# Correction automatique
./scripts/fix-lint.sh

# PrÃ©visualiser sans modifier
./scripts/fix-lint.sh --dry-run
```

### Bot Telegram

```bash
# Statut du service
sudo systemctl status telegram-bot

# Logs en temps rÃ©el
sudo journalctl -u telegram-bot -f

# RedÃ©marrer
sudo systemctl restart telegram-bot

# VÃ©rifier le fichier .env
sudo cat /opt/services/telegram-bot/.env
# â†’ Doit contenir TELEGRAM_BOT_TOKEN et ALLOWED_CHAT_IDS
```

### Monit

```bash
# VÃ©rifier la syntaxe (TOUJOURS faire Ã§a aprÃ¨s modification)
sudo monit -t

# Statut de tous les services
sudo monit status

# Relancer Monit
sudo systemctl restart monit

# Voir les logs Monit
sudo tail -f /var/log/monit.log
```

### WSL2 (dÃ©veloppement local)

```bash
# Si DNS ne fonctionne pas sous WSL2
ansible-playbook playbooks/wsl-repair.yml --connection local

# RedÃ©marrer WSL (dans PowerShell)
wsl --shutdown
```

### Conteneurs Docker

```bash
# Voir tous les conteneurs
docker ps -a

# Logs d'un conteneur
docker logs headscale --tail 50

# RedÃ©marrer un conteneur
docker restart headscale

# VÃ©rifier le rÃ©seau proxy-net
docker network inspect proxy-net
```

---

## Messages d'erreur courants et leurs solutions

| Message d'erreur | Cause probable | Solution |
|-----------------|---------------|----------|
| `undefined variable: 'domain_...'` | Variables manquantes dans converge.yml | Ajouter bloc `vars:` dans le converge.yml |
| `docker: command not found` | Docker pas installÃ© dans le conteneur Molecule | Ajouter `prepare.yml` avec installation Docker |
| `collection 'community.docker' not found` | `ansible-galaxy` non exÃ©cutÃ© | `ansible-galaxy collection install -r requirements.yml` |
| `ACME challenge failed` | DNS ne pointe pas vers le serveur | VÃ©rifier les DNS ou activer `ci_mode=true` |
| `Container exited with code 1` (Zerobyte) | `APP_SECRET` manquant ou mal formatÃ© | `openssl rand -hex 32` â†’ 64 hex chars |
| `monit: syntax error` | CaractÃ¨res spÃ©ciaux dans le mot de passe | Mot de passe alphanumÃ©rique uniquement |
| `healthcheck: UNHEALTHY` (Headplane) | Healthcheck sur image distroless | Supprimer le healthcheck Docker |
| `502 Bad Gateway` (Caddy â†’ backend) | Le conteneur backend n'est pas sur proxy-net | VÃ©rifier le rÃ©seau dans docker-compose.yml |
| `permission denied` (.env telegram) | Fichier .env pas en mode 600 | `chmod 600 /opt/services/telegram-bot/.env` |
| `vault password not found` | OubliÃ© `--ask-vault-pass` | Ajouter `--ask-vault-pass` Ã  la commande |
| `set: Illegal option -o pipefail` | Dash utilisÃ© au lieu de Bash | Ajouter `executable: /bin/bash` Ã  la tÃ¢che shell |
| `lsb_release: command not found` | `lsb-release` absent sur Debian 13 | Ajouter `lsb-release` dans `prepare.yml` |
| `No module named 'requests'` | `python3-requests` absent | Ajouter `python3-requests` dans `prepare.yml` |
| `mount destination is not a directory` | Bind mount de fichier en DinD | Monter des RÃ‰PERTOIRES, jamais des fichiers |
| `Flag --datacenter is deprecated` | hcloud CLI v1.59+ | Remplacer `--datacenter` par `--location` |
| `callback plugin has been removed` | `community.general` v12 | Utiliser `ansible.builtin.default` + `result_format: yaml` |
| `service is activating (not active)` | Service pas encore dÃ©marrÃ© | Ajouter `retries` + `delay` dans le verify |
| `initial DERPMap is empty` | DERP desactive + `urls: []` | Activer le DERP embarque (`derp.server.enabled: true`) |
| `server_url cannot use same domain as base_domain` | Meme domaine pour API et Magic DNS | Variable `headscale_base_domain` separee de `domain_headscale` |
| `failed to save private key to disk at path ""` | `derp.server.private_key_path` manquant | Ajouter le chemin dans la config DERP |
| `Cannot convert undefined or null to object` (Headplane) | Champs DNS manquants dans config Headscale | Ajouter `split: {}`, `search_domains: []`, `extra_records: []` |
| `Timed out waiting for Headscale API` (408) | Headplane demarre avant Headscale | `docker restart headplane` apres restart Headscale |
| `invalid argument for --user flag` | Headscale 0.26.0 veut un ID numerique | `headscale users list` pour obtenir l'ID |
| `Skipping monitor configuration` (Uptime Kuma) | Username case-sensitive (`admin` â‰  `Admin`) | Corriger `uptime_kuma_admin_username` dans le vault |
| Sonde Headscale API DOWN (Uptime Kuma) | Headscale 0.26 renvoie 404 sur `/` | Pointer la sonde vers `/api/v1/apikey` (attend 401) |
