# ğŸ—ºï¸ Feuille de route V4 â€” Axes d'amÃ©lioration

> Ce document prÃ©sente les amÃ©liorations prÃ©vues pour la V4 de Seko-VPN, avec leur priorisation, leur effort estimÃ© et leur architecture cible. Ces Ã©lÃ©ments Ã©taient **hors pÃ©rimÃ¨tre V3** par choix.

---

## Ce qui a Ã©tÃ© rÃ©alisÃ© en V3

Avant de parler du futur, rappelons ce que V3 a ajoutÃ© par rapport Ã  V2 :

| # | AmÃ©lioration | Statut |
|---|-------------|--------|
| 1 | Hardening sysadmin (journald, logrotate, chrony, swap, unattended-upgrades) | âœ… V3 |
| 2 | Uptime Kuma + Monit headless | âœ… V3 |
| 3 | Grafana Alloy prÃ©-installÃ© | âœ… V3 |
| 4 | Bot Telegram interactif | âœ… V3 |
| 5 | Wizard de configuration | âœ… V3 |
| 6 | CI mode local_certs | âœ… V3 |
| 7 | Docker prune timer | âœ… V3 |
| 8 | Kernel panic auto-reboot | âœ… V3 |
| 9 | **Pipeline CI/CD 3 stages validÃ©** (lint + 14 rÃ´les Molecule + intÃ©gration Hetzner) | âœ… V3.0.0 |
| 10 | **42 piÃ¨ges documentÃ©s** (REX Day 0 â†’ Round 5) | âœ… V3.0.0 |
| 11 | **Bonnes pratiques DinD** (bind mounts, idempotence, Debian 13) | âœ… V3.0.0 |

---

## Matrice de priorisation V4

| # | AmÃ©lioration | Impact | Effort | PrioritÃ© |
|---|-------------|--------|--------|----------|
| 1 | SSO/OIDC centralisÃ© | ğŸ”´ Ã‰levÃ© (sÃ©curitÃ©) | 2-3 jours | ğŸ”´ Haute |
| 2 | Backup chiffrÃ© vers S3 | ğŸ”´ Ã‰levÃ© (rÃ©silience) | 1-2 jours | ğŸ”´ Haute |
| 3 | Multi-serveur Telegram | ğŸ”´ Ã‰levÃ© (opÃ©rations) | 2-3 jours | ğŸ”´ Haute |
| 3b | Security Hardening V3.2 | ğŸ”´ Ã‰levÃ© (sÃ©curitÃ©) | 2-3 jours | ğŸ”´ Haute |
| 4 | Scanning de vulnÃ©rabilitÃ©s (Trivy) | ğŸŸ¡ Moyen (sÃ©curitÃ©) | 1 jour | ğŸŸ¡ Moyenne |
| 5 | Support multi-OS | ğŸŸ¡ Moyen (portabilitÃ©) | 3-5 jours | ğŸŸ¡ Moyenne |
| 6 | Stack observabilitÃ© complÃ¨te | ğŸŸ¡ Moyen (visibilitÃ©) | 3-4 jours | ğŸŸ¡ Moyenne |
| 7 | Rotation automatique des secrets | ğŸŸ¢ Faible (sÃ©curitÃ©) | 1-2 jours | ğŸŸ¢ Basse |
| 8 | Tests Molecule avancÃ©s | ğŸŸ¢ Faible (qualitÃ©) | 2 jours | ğŸŸ¢ Basse |
| 9 | Wizard GUI web | ğŸŸ¢ Faible (UX) | 2-3 jours | ğŸŸ¢ Basse |

> **Recommandation :** Commencer par SSO (1) + Backup S3 (2) + Multi-serveur Telegram (3). Ces trois Ã©lÃ©ments transforment la stack d'un dÃ©ploiement mono-serveur en une plateforme multi-serveurs sÃ©curisÃ©e et rÃ©siliente.

---

## 1. SSO/OIDC centralisÃ©

**PrioritÃ© : ğŸ”´ Haute** Â· **Effort : 2-3 jours**

### ProblÃ¨me actuel

Chaque service a son propre systÃ¨me d'authentification. L'opÃ©rateur doit gÃ©rer N mots de passe diffÃ©rents. Pas de 2FA centralisÃ©. Pas de rÃ©vocation centralisÃ©e des accÃ¨s.

### Solution V4

DÃ©ployer **Authelia** comme Identity Provider (IdP) centralisÃ© :

```
Client HTTPS
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Caddy                                                    â”‚
â”‚  â”œâ”€â”€ forward_auth authelia:9091 â”€â”€â–º Authelia            â”‚
â”‚  â”‚       â”‚                          (vÃ©rifie l'identitÃ©)â”‚
â”‚  â”‚       â”‚ âœ… AuthentifiÃ©                                â”‚
â”‚  â”‚       â–¼                                               â”‚
â”‚  â”œâ”€â”€ portainer.example.com â†’ Portainer                  â”‚
â”‚  â”œâ”€â”€ zb.example.com â†’ Zerobyte                          â”‚
â”‚  â””â”€â”€ status.example.com â†’ Uptime Kuma                   â”‚
â”‚                                                          â”‚
â”‚  Services avec SSO natif (pas de forward_auth) :         â”‚
â”‚  â”œâ”€â”€ vault.example.com â†’ Vaultwarden (OIDC natif)       â”‚
â”‚  â””â”€â”€ nga.example.com â†’ Headplane (OIDC natif)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Ce que Ã§a apporte

- Un seul login pour tous les services
- 2FA centralisÃ© (TOTP, WebAuthn)
- RÃ©vocation instantanÃ©e d'un accÃ¨s
- Audit log centralisÃ© des connexions
- Nouveau rÃ´le Ansible : `authelia`

---

## 2. Backup chiffrÃ© vers S3

**PrioritÃ© : ğŸ”´ Haute** Â· **Effort : 1-2 jours**

### ProblÃ¨me actuel

Zerobyte est installÃ© mais non configurÃ© avec un backend de stockage distant. Si le VPS est perdu (crash disque, erreur hÃ©bergeur), toutes les donnÃ©es sont perdues.

### Solution V4

PrÃ©-configurer Zerobyte avec un repository S3 (Backblaze B2 ou Wasabi) :

```
Zerobyte (VPS) â”€â”€â–º Chiffrement local â”€â”€â–º S3 (Backblaze B2 / Wasabi)
                                              â”‚
                                              â”œâ”€â”€ Backup quotidien incrÃ©mental
                                              â”œâ”€â”€ Backup hebdomadaire complet
                                              â””â”€â”€ RÃ©tention : 30 jours
```

### Variables Ã  ajouter dans vault.yml

```yaml
vault_s3_access_key: "xxx"
vault_s3_secret_key: "xxx"
vault_s3_bucket: "seko-vpn-backups"
vault_s3_endpoint: "s3.eu-central-003.backblazeb2.com"
```

### Monitoring des backups

Ajouter un check Monit qui vÃ©rifie le timestamp du dernier backup :
- Si le dernier backup date de plus de 48h â†’ alerte Telegram
- Test de restauration automatisÃ© dans le pipeline CI

---

## 3. Multi-serveur Telegram Bot

**PrioritÃ© : ğŸ”´ Haute** Â· **Effort : 2-3 jours**

### ProblÃ¨me actuel

Le bot Telegram V3 est local Ã  un seul serveur. Si tu gÃ¨res 3 VPS, tu as 3 bots diffÃ©rents sur 3 conversations Telegram distinctes.

### Solution V4

```
Telegram â†’ Bot central (VPS dÃ©diÃ© ou serverless)
              â”‚
              â”œâ”€â”€ /status vps1  â†’ Agent VPS1 (via Headscale VPN)
              â”œâ”€â”€ /status vps2  â†’ Agent VPS2 (via Headscale VPN)
              â”œâ”€â”€ /status all   â†’ Tous les agents
              â””â”€â”€ /restart vps1 caddy â†’ Agent VPS1 (avec confirmation)
```

### Architecture

- **Bot central** : reÃ§oit les commandes Telegram, dispatch vers les agents
- **Agents lÃ©gers** : API REST (FastAPI) sur chaque VPS, communiquent via le VPN Headscale
- **SÃ©curitÃ©** : les agents ne sont PAS exposÃ©s sur internet, ils Ã©coutent uniquement sur le rÃ©seau VPN

### PrÃ©paration V3

Le bot V3 est dÃ©jÃ  prÃ©parÃ© pour Ã§a : chaque rÃ©ponse est prÃ©fixÃ©e par `[nom-du-serveur]` (variable `telegram_bot_server_name`).

---

## 3b. Security Hardening V3.2

**PrioritÃ© : ğŸ”´ Haute** Â· **Effort : 2-3 jours** Â· **PrÃ©-requis : second serveur VPN opÃ©rationnel**

### Contexte

Audit de sÃ©curitÃ© rÃ©alisÃ© post-V3.1. Le serveur est fonctionnel mais prÃ©sente des surfaces d'attaque rÃ©ductibles. Ce durcissement doit Ãªtre dÃ©ployÃ© **aprÃ¨s** avoir validÃ© le VPN multi-serveur (A â†’ B â†’ C) pour Ã©viter de se verrouiller dehors.

### 9 axes de durcissement (par ordre de dÃ©ploiement)

| # | Axe | Risque | Effort | Impact |
|---|-----|--------|--------|--------|
| 1 | **Pin images `latest`** | Risque zÃ©ro | 10 min | Headplane et Uptime Kuma utilisent `latest`. Pinner sur un tag prÃ©cis empÃªche les rÃ©gressions silencieuses |
| 2 | **Container capability dropping** | Faible | 30 min | Ajouter `cap_drop: ALL` + `cap_add` minimaux dans chaque `docker-compose.yml.j2`. RÃ©duit la surface d'attaque en cas de compromission d'un conteneur |
| 3 | **Fail2Ban HTTP jails** | Faible | 1h | Ajouter des jails Fail2Ban pour les 401/403 Caddy. Bloque le brute-force sur les interfaces web (Portainer, Vaultwarden, Headplane) |
| 4 | **Rate limiting Caddy** | Faible | 1h | Limiter les requÃªtes par IP sur les vhosts sensibles (login pages). ComplÃ¨te Fail2Ban |
| 5 | **ACL Headscale granulaires** | Moyen | 2h | Remplacer la policy `allow-all` par des groups/roles. ContrÃ´ler qui peut communiquer avec qui dans le VPN |
| 6 | **Docker socket proxy** | Moyen | 2h | Remplacer le bind mount `/var/run/docker.sock` par un proxy Tecnativa en read-only. ProtÃ¨ge Portainer et Alloy |
| 7 | **Segmentation rÃ©seau Docker** | Moyen | 2h | Ajouter un rÃ©seau `mgmt-net` dÃ©diÃ© Ã  Portainer + socket-proxy (Tecnativa). Conditionnel Ã  l'axe 6 (socket proxy). Seko-VPN n'a pas de DB partagÃ©e : un seul rÃ©seau `proxy-net` suffit pour les 7 services applicatifs |
| 8 | **SSH via VPN uniquement** | Ã‰levÃ© | 1h | Restreindre UFW pour n'autoriser SSH que depuis le subnet VPN (100.64.0.0/10). **PrÃ©-requis** : VPN multi-serveur validÃ© + porte de secours IPMI/KVM |
| 9 | **AppArmor profiles** | Optionnel | 4h+ | Profils AppArmor custom par conteneur. Complexe, bÃ©nÃ©fice marginal si les autres axes sont en place |

### DÃ©ploiement recommandÃ©

```
Phase 1 (immÃ©diat, zÃ©ro risque) : axes 1-2
Phase 2 (protection rÃ©seau)     : axes 3-4-5
Phase 3 (isolation avancÃ©e)     : axes 6-7
Phase 4 (verrouillage final)    : axe 8 (aprÃ¨s multi-serveur)
Phase 5 (optionnel)             : axe 9
```

### Fichiers principaux impactÃ©s

- `roles/*/templates/docker-compose.yml.j2` â€” cap_drop, rÃ©seaux
- `roles/security/tasks/main.yml` â€” Fail2Ban jails, UFW VPN-only
- `roles/caddy/templates/Caddyfile.j2` â€” rate limiting
- `roles/headscale/templates/policy.json.j2` â€” ACL granulaires
- `roles/docker/tasks/main.yml` â€” crÃ©ation du rÃ©seau `mgmt-net`, socket proxy

---

## 4. Scanning de vulnÃ©rabilitÃ©s (Trivy)

**PrioritÃ© : ğŸŸ¡ Moyenne** Â· **Effort : 1 jour**

### Ce que Ã§a fait

IntÃ©grer Trivy dans le pipeline CI pour scanner les 8 images Docker Ã  chaque merge sur `main` :

```
Merge sur main
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Stage : Security Scan                   â”‚
â”‚                                         â”‚
â”‚ trivy image headscale/headscale:0.26.0 â”‚
â”‚ trivy image vaultwarden/server:1.35.1  â”‚
â”‚ trivy image caddy:latest               â”‚
â”‚ ... (8 images)                          â”‚
â”‚                                         â”‚
â”‚ Si CRITICAL â†’ âŒ Pipeline bloquÃ©        â”‚
â”‚ Si HIGH â†’ âš ï¸ Warning (ne bloque pas)   â”‚
â”‚                                         â”‚
â”‚ Rapport Markdown â†’ artefact CI          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 5. Support multi-OS

**PrioritÃ© : ğŸŸ¡ Moyenne** Â· **Effort : 3-5 jours**

### OS cibles

| OS | PrioritÃ© | DifficultÃ© | Notes |
|----|----------|-----------|-------|
| Ubuntu 22.04/24.04 | P1 | Faible | Noms de paquets similaires, dÃ©pÃ´ts Docker identiques |
| Rocky Linux 9 | P2 | Moyenne | `dnf` au lieu de `apt`, `firewalld` au lieu de UFW, SELinux |

### Impact sur le projet

- Conditionner les tÃ¢ches avec `ansible_os_family` ou `ansible_distribution`
- Ã‰tendre la matrice Molecule : `debian12 + ubuntu2404`
- Adapter les noms de paquets et chemins de config

---

## 6. Stack observabilitÃ© complÃ¨te

**PrioritÃ© : ğŸŸ¡ Moyenne** Â· **Effort : 3-4 jours**

### Architecture cible

Grafana Alloy est dÃ©jÃ  en place (V3). Il suffit de dÃ©ployer le backend :

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ VPS Seko-VPN     â”‚     â”‚ VPS Monitoring   â”‚
â”‚                  â”‚     â”‚ (dÃ©diÃ©)          â”‚
â”‚ Grafana Alloy â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Loki (logs)    â”‚
â”‚ cAdvisor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Prometheus     â”‚
â”‚                  â”‚     â”‚                  â”‚
â”‚                  â”‚     â”‚ Grafana â—„â”€â”€ dashboards + alertes
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Nouveaux composants

- **Loki** : stockage et indexation des logs (receveur pour Alloy)
- **Prometheus** : mÃ©triques systÃ¨me et conteneurs
- **cAdvisor** : mÃ©triques dÃ©taillÃ©es des conteneurs Docker
- **Grafana** : dashboards et alertes visuelles

### Activation sur V3 existant

Pour connecter un VPS V3 existant Ã  un serveur Loki :

```bash
ansible-vault edit inventory/group_vars/all/vars.yml
# Modifier : alloy_loki_url: "http://loki.monitoring.example.com:3100/loki/api/v1/push"
ansible-playbook playbooks/site.yml --tags alloy --ask-vault-pass
```

---

## 7. Rotation automatique des secrets

**PrioritÃ© : ğŸŸ¢ Basse** Â· **Effort : 1-2 jours**

### Concept

Un playbook `rotate-secrets.yml` qui :

1. GÃ©nÃ¨re de nouveaux secrets (mÃªmes contraintes que wizard.sh)
2. Met Ã  jour `vault.yml`
3. RedÃ©marre les services impactÃ©s dans le bon ordre
4. VÃ©rifie que tout fonctionne
5. Notifie via Telegram
6. Log l'opÃ©ration

---

## 8. Tests Molecule avancÃ©s

**PrioritÃ© : ğŸŸ¢ Basse** Â· **Effort : 2 jours**

### AmÃ©liorations prÃ©vues

- **Tests Testinfra** (Python) en plus des assertions Ansible â€” plus de flexibilitÃ©
- **Test d'idempotence** : double converge, vÃ©rifier `changed=0` au second run
- **Matrice Ã©tendue** : Debian 12 / Debian 13 dans le pipeline
- **ScÃ©nario multi-rÃ´les** : les 14 rÃ´les enchaÃ®nÃ©s dans l'ordre (comme `site.yml`)

---

## 9. Wizard GUI web

**PrioritÃ© : ğŸŸ¢ Basse** Â· **Effort : 2-3 jours**

Alternative au wizard.sh CLI : une page HTML simple servie localement avec :
- Formulaire avec tous les champs
- Validation en temps rÃ©el (ex: vÃ©rifier que l'IP est valide)
- GÃ©nÃ©ration des fichiers YAML
- Affichage des enregistrements DNS Ã  crÃ©er

DestinÃ© aux opÃ©rateurs moins techniques qui prÃ©fÃ¨rent une interface graphique.

---

## RÃ©sumÃ© de l'Ã©volution V1 â†’ V4

| MÃ©trique | V1 | V2 | V3 | V4 (cible) |
|----------|----|----|-----|------------|
| RÃ´les Ansible | 10 | 10 | 14 | 15-16 |
| Services Docker | 7 | 7 | 8 | 9-10 |
| Services natifs | 1 | 1 | 3 | 3-4 |
| ScÃ©narios Molecule | 0 | 10 | 14 | 16+ |
| Pipeline CI stages | 0 | 3 | 3 | 4 (+ security) |
| OS supportÃ©s | 1 | 1 | 1 | 2-3 |
| Authentification | Individuelle | Individuelle | Individuelle | SSO centralisÃ© |
| Backup S3 | âŒ | âŒ | âŒ | âœ… |
| Multi-serveur | âŒ | âŒ | PrÃ©parÃ© | âœ… |
